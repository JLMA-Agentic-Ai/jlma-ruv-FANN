# cuda-wasm Kubernetes Deployment for Nutanix NKE
#
# This manifest deploys cuda-wasm GPU workloads on Nutanix Kubernetes Engine (NKE).
# It includes:
#   - Namespace for isolation
#   - ConfigMap for runtime settings
#   - Nutanix CSI PVC for kernel cache storage
#   - Deployment with NVIDIA GPU requests and node affinity
#   - Service for API access
#   - HorizontalPodAutoscaler for GPU-based scaling
#
# Usage:
#   kubectl apply -f kubernetes_deployment.yaml
#
# Prerequisites:
#   - NKE cluster with GPU-enabled node pool
#   - NVIDIA GPU Operator or device plugin installed
#   - Nutanix CSI driver configured with storage class "nutanix-volume"
#
---
apiVersion: v1
kind: Namespace
metadata:
  name: cuda-wasm
  labels:
    app.kubernetes.io/part-of: cuda-wasm
    platform: nutanix-nke
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cuda-wasm-config
  namespace: cuda-wasm
  labels:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/component: config
data:
  # GPU backend selection: cuda, rocm, oneapi, webgpu
  CUDA_WASM_GPU_BACKEND: "cuda"
  # Number of GPUs to use per worker
  CUDA_WASM_GPU_COUNT: "1"
  # Directory for compiled kernel cache (mounted via PVC)
  CUDA_WASM_KERNEL_CACHE_DIR: "/cache/kernels"
  # Logging level: trace, debug, info, warn, error
  CUDA_WASM_LOG_LEVEL: "info"
  # Enable WebGPU fallback when native GPU is unavailable
  CUDA_WASM_WEBGPU_ENABLED: "true"
  # Memory pool size for GPU allocations (bytes)
  CUDA_WASM_MEMORY_POOL_SIZE: "2147483648"
  # Maximum concurrent kernel executions
  CUDA_WASM_MAX_CONCURRENT_KERNELS: "16"
  # Transpiler optimization level (0-3)
  CUDA_WASM_OPT_LEVEL: "3"
  # Enable kernel profiling
  CUDA_WASM_PROFILING: "false"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cuda-wasm-kernel-cache
  namespace: cuda-wasm
  labels:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/component: cache
  annotations:
    # Nutanix CSI volume annotations for NKE
    csi.nutanix.com/storage-type: "NutanixVolumes"
    # Optional: specify Nutanix storage container
    # csi.nutanix.com/storage-container: "default-container"
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: nutanix-volume
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cuda-wasm-worker
  namespace: cuda-wasm
  labels:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/instance: cuda-wasm-worker
    app.kubernetes.io/component: gpu-worker
    app.kubernetes.io/part-of: cuda-wasm
    app.kubernetes.io/managed-by: cuda-wasm-deployer
    cuda-wasm/gpu-vendor: nvidia
  annotations:
    # NKE-specific annotations
    nke.nutanix.com/gpu-enabled: "true"
    nke.nutanix.com/cluster-type: "gpu-workload"
    # Optional: specify GPU driver version
    # nke.nutanix.com/gpu-driver: "nvidia-535"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: cuda-wasm-worker
      app.kubernetes.io/instance: cuda-wasm-worker
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cuda-wasm-worker
        app.kubernetes.io/instance: cuda-wasm-worker
        app.kubernetes.io/component: gpu-worker
        cuda-wasm/gpu-vendor: nvidia
    spec:
      # Node affinity to schedule on GPU-equipped nodes
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  # Require NVIDIA GPU presence (set by GPU device plugin)
                  - key: nvidia.com/gpu.present
                    operator: In
                    values:
                      - "true"
              - matchExpressions:
                  # Alternative: NFD-detected PCI device
                  - key: feature.node.kubernetes.io/pci-10de.present
                    operator: In
                    values:
                      - "true"
          preferredDuringSchedulingIgnoredDuringExecution:
            # Prefer nodes labeled for cuda-wasm workloads
            - weight: 100
              preference:
                matchExpressions:
                  - key: cuda-wasm/gpu-vendor
                    operator: In
                    values:
                      - "nvidia"
        # Anti-affinity: spread workers across hosts
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 50
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - cuda-wasm-worker
                topologyKey: kubernetes.io/hostname
      # Tolerate GPU node taints
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: "node-role.kubernetes.io/gpu"
          operator: Exists
          effect: NoSchedule
      containers:
        - name: cuda-wasm-worker
          image: registry.example.com/cuda-wasm:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
            - containerPort: 9090
              name: metrics
              protocol: TCP
          envFrom:
            - configMapRef:
                name: cuda-wasm-config
          env:
            # GPU-specific environment
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            # Pod identity for distributed workloads
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          resources:
            requests:
              cpu: "2000m"
              memory: "8Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "8000m"
              memory: "32Gi"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: kernel-cache
              mountPath: /cache/kernels
            # Shared memory for NCCL and inter-process communication
            - name: dshm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 30
      volumes:
        - name: kernel-cache
          persistentVolumeClaim:
            claimName: cuda-wasm-kernel-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 8Gi
      # Optional: use Nutanix-specific runtime class
      # runtimeClassName: nvidia
---
apiVersion: v1
kind: Service
metadata:
  name: cuda-wasm-worker
  namespace: cuda-wasm
  labels:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/component: api
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/instance: cuda-wasm-worker
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cuda-wasm-worker-hpa
  namespace: cuda-wasm
  labels:
    app.kubernetes.io/name: cuda-wasm-worker
    app.kubernetes.io/component: autoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cuda-wasm-worker
  minReplicas: 1
  maxReplicas: 8
  metrics:
    # Scale based on CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    # Scale based on GPU utilization (requires DCGM exporter + custom metrics)
    - type: Pods
      pods:
        metric:
          name: nvidia_com_gpu_utilization
        target:
          type: AverageValue
          averageValue: "70"
    # Scale based on pending work queue (custom metric)
    - type: Pods
      pods:
        metric:
          name: cuda_wasm_pending_kernels
        target:
          type: AverageValue
          averageValue: "10"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120
